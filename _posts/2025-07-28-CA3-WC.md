---
layout: post
title: "Core Area 3: The Wider Context"
author: "Eric Martin"
date: 2025-07-28
categories: documentation
image: Turingbw.jpg
---
<p style="font-size: 1.75em; font-weight: bold; text-align: center;">
<em>“To invent a new technology is to open a Pandora’s box. Once a particular device or system is widely adopted, it begins to reshape practices and possibilities in its own image.”</em>
</p>

<p style="text-align: right; font-size: 1.25em;">
<em>— Langdon Winner</em>
</p>

## Preface

## Core Area 3: GDPR and Data Ethics in Digital Education: Insights from Turing DSG

As Principal Investigator for the September 2025 Turing Data Study Group challenge, I hold formal responsibility not only for the scientific direction and educational value of the project, but also for ensuring ethical and legal compliance with the <a href="https://www.legislation.gov.uk/ukpga/2018/12/contents">UK Data Protection Act 2018</a> and the <a href="https://www.legislation.gov.uk/eur/2016/679/contents">UK General Data Protection Regulation (UK GDPR)</a>. This includes authoring and clearing the project ethics submission under the Alan Turing Institute’s TREx framework, which is mandatory for all Eligible Research involving the Institute. Every DSG challenge requires a detailed ethics submission (TREx Form 1 and 2), documenting risk mitigation, data provenance, and alignment with both institutional and legal standards. My work is guided by the <a href="https://www.turing.ac.uk/turing-research-ethics-policy-trex-0">The Alan Turing Institute Research Ethics Policy (TREx)</a>, and enforced by <a href="https://www.turing.ac.uk/research-misconduct-policy">The Alan Turing Institute Research Misconduct Policy</a>, ensuring that data handling, risk assessment, and transparency requirements are met throughout the project lifecycle.

As Principal Investigator, I am solely responsible for ensuring that our project meets the ethical and legal standards required under this framework. The process is rigorous. It begins with a self-assessment (TREx Form 1), followed by a detailed submission (TREx Form 2) that undergoes internal triage, review, and panel evaluation. Each submission must address research purpose, potential harms, data provenance, risk mitigation, and any implications for human participants or sensitive material. Conditions or clarifications may be requested, and approval is contingent on full compliance. The TREx framework aligns closely with the Institute’s Data Protection Assessment Process, and operates alongside its research misconduct protocols, ensuring integrity at every stage of the research lifecycle. The burden of this process is real and necessary. It formalises responsibilities that researchers should already take seriously, and it forces early reflection on potential ethical and practical failings that might otherwise go unaddressed.

The TREx ethics submission is a rigorous engagement across four principled domains: Fairness, Accountability, Sustainability, and Transparency, each broken into specific obligations. Under Fairness, I had to assess whether model outputs might favour particular sea conditions, wreck types, or geographies, and describe how we would mitigate these structural biases through training data selection and temporal cross-validation. Under Accountability, I was required to document governance structures, version control, and reproducibility mechanisms, from GitHub protocols to shared Overleaf records. Sustainability included both environmental impact and stakeholder positioning, requiring a statement of downstream effects and safeguards against unintended use. Finally, Transparency demanded a plan for public reporting, reproducibility, and the interpretability of outcomes by non-technical stakeholders. Every section required concrete answers about metadata, failure modes, bias sources, and review processes. My assessment of these factors in relation to the use of technologies in this live-data educational reserach, once approved, become auditable commitments.



***Core Area 3 Evidence***

<a href="/assets/img/TuringEC2.pdf" target="_blank" style="text-decoration: underline; color: #004080;">-(Evidence 1) Alan Turing Institute Employment Confirmation</a>

<a href="https://www.turing.ac.uk/sites/default/files/2019-08/dsg_pi_guide_0.pdf" target="_blank" style="text-decoration: underline; color: #004080;">-(Evidence 2) Alan Turing Institute PI Guide, Including Ethics Duties</a>

<a href="https://www.turing.ac.uk/research-misconduct-policy" target="_blank" style="text-decoration: underline; color: #004080;" >-(Evidence 3) Alan Turing Institute Research Misconduct Policy</a>

<a href="https://www.turing.ac.uk/turing-research-ethics-policy-trex-0" target="_blank" style="text-decoration: underline; color: #004080;">-(Evidence 4) Alan Turing Institute Research Ethics Policy (TREx)</a>

***Core Area 3 Reflections***

Before taking on a Principal Investigator role under the Alan Turing Institute’s TREx framework, I had a working understanding of GDPR and a respect for data protection. But TREx reframed that entirely. It didn’t just ask whether our project complied with the law but also demanded a full account of how data would be handled, stored, accessed, and interpreted, from first receipt to final publication.

This meant confronting issues I had never formalised: whether environmental datasets contained hidden licensing restrictions; whether SAR imagery carried military sensitivity or dual-use implications; whether our machine learning outputs could influence commercial valuations or be used for unintended surveillance. It wasn’t just about what the model would predict, but what someone else might infer from it.

The TREx submission required concrete answers. Who owned the code? Who owned the training data? Could any of it be re-identified? What happened after the study closed? Access had to be locked down, with named collaborators only. All development was sandboxed within the Alan Turing Institute’s Trusted Research Environment (TRE), an ephemeral workspace with no external access, version-tracked commits, and an automatic teardown process that ensures no residual data or credentials persist once the work concludes.

We had to define which repositories would hold what data, which parts would be made public, and how results would be communicated without misinterpretation. That included publishing metadata protocols, documenting potential biases, and describing mitigation strategies upfront. Archive planning was part of the ethics submission. So was public interpretability. Complete and detailed transparency was requisite.

This process was new to me, and even though it was time consuming, it didn’t slow the work. It defined it. And it altered how I approach TEL and research teaching. I no longer see educational data tools or student projects as low-stakes or sealed-off. Every dataset carries an origin, an owner, and a future as outputs and algorithms. As an educator, and in this case, as so often is the case, also the student, I realised that when working with data we’re not just teaching skills, but also shaping how future researchers handle responsibility, which begins with design, not delivery.

